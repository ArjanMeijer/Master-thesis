\documentclass[../../Thesis.tex]{subfiles}
\begin{document}
\header{Research Data}
\subheader{Corpus}
The dataset we used for this research consists of articles published in 2017 which have been published in journals that have, in 2017, atleast 150 publications. This results in a total dataset of $1.391.543$ articles from $3.759$ journals. Details about the corpus can be found in table~\ref{table:corpusSize}.
\begin{table}[hbt]
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
 & Total count & Unique count & Average length\footnotemark \\
\hline\hline
Title words & $18.822.399$ & $939.665$ & 14  \\
\hline
Title tokens & $14.742.192$ & $230.805$ & 11 \\
\hline\hline
Abstract words & $264.653.020$ & $5.853.077$  & 190  \\
\hline
Abstract tokens & $171.474.473$ & $738.961$ & 124 \\
\hline\hline
Total words & $283.475.419$ & $6.209.769$  & 204 \\
\hline
Total tokens & $186.962.354$ & $763.475$ & 134 \\
\hline
\end{tabular}
\end{center}
\caption{Corpus size}\label{table:corpusSize}
\end{table}
\footnotetext{Rounded}
\clearpage
\subsubheader{Text properties}
\citet{wiegand2018word} state that the Pareto distribution offers a good fit to the word occurrences in natural language. Their models show that, due to the additional parameters in the Pareto-III\footnote{The pareto distribution is a distribution in which recognizable by a long "tail" of low values and a quick and short ascend to the top values. More information: \url{https://en.wikipedia.org/wiki/Pareto_distribution}.}, the tail of the data fits better with the model than the Zipf\footnote{Distribution similar to the Pareto distribution, see: \url{https://en.wikipedia.org/wiki/Zipf\%27s_law}} model. This shows the relation between the word occurrences rank and the actual occurrences. This kind of word-occurrences distribution holds for many texts, including the writings of William Shakespeare, scientific texts and novels\cite{thurner2015understanding}.
The word occurrences in our corpus also follow the pattern of a Pareto distribution as described by~\citet{wiegand2018word}. Our distribution is visualized in Figure~\ref{figure:wordTokenOccurrence}, which displays the occurrences of the first 500 tokens of the corpus.
\FloatBarrier
\begin{figure}[hbt]
\includegraphics[width=6.5in]{Plots/word_occurrences}
\caption{Word and token occurrences. The word counts pattern resembles a Pareto distribution, the tokenization reduced the amount of high frequency words.}\label{figure:wordTokenOccurrence}
\end{figure}
\FloatBarrier
\subheader{Datasets}
For this researched we used a (pre-made) tokenized dataset, created from the earlier described corpus, which reduces the total amount of words by 34\% (see table~\ref{table:corpusSize}). From this tokenized set, we created the embeddings and the TF-IDF feature vectors.
\subsubheader{Tokenization}
The following steps have been applied to the words to create a tokenized set:
\begin{enumerate}
\item{Removed punctuation}
\item{Removed all non-ASCII characters}
\item{Transformed all characters to lower-case}
\item{Removed stop-words, as provided by the NLTK\footnote{Natural Language ToolKit, \url{https://www.nltk.org/}} library}
\item{Removed numbers}
\item{Stemmed all words, using the stemmer provided by the NLTK library}
\end{enumerate}
These transformations reduced our dataset by 34\%, resulting in a tokenized set of $186.962.354$ tokens.
\subsubheader{Embedding}
For this research, we reused the word embeddings created by \citet{Truong2017Thesis}. These embeddings have a vector dimension of 300, which is an industry default. They have been trained on the entire Elsevier corpus, not limited to the subset we used for this research. To create higher-level embeddings, we average all component embeddings. Thus, to create article embeddings, we take the average of all normalized word embeddings for that article. Journal embeddings are created by taking the average of all normalized article embeddings. We use the average to combine multiple embeddings into one, because we determine the meaning of a larger text as the average meaning of all components. Since this meaning is represented as an embedding, we can average the embeddings to create the "average meaning". We have used multiple embedding optimizations for this research.
\subsubsubheader{Default embedding}
The default embedding is created form the pre-trained word embeddings; no modifications have been applied to this set. We refer to this set in the figures as \textbf{embedding}.
\subsubsubheader{TF-IDF weighted embedding}
The TF-IDF weighted embedding set, referred to as TF-IDF embedding, are the default word embeddings weighted with a TF-IDF score per word.\\
The TF-IDF is calculated with a raw token count, and a smoothed inverted document frequency, calculated as follows:\\
\begin{equation}
IDF = \log_{10}(\dfrac{|A|}{|A_t|})
\end{equation}
Where $|A|$ is the total count of articles and $|A_t|$ is the count of articles containing term t.
The articles embeddings are a normalized summation of each word vector multiplied by it TF-IDF value. Since we take a sum of all words, the Term Frequency is embedded as the raw count of each word. We will refer to this embedding in the figures as \textbf{tfidf\_embedding}.
\subsubsubheader{10K TF-IDF embedding}
The 10K embedding set is generated similarity to the TF-IDF embedding, this version only uses the 10.000 most common tokens, reducing the number of tokens it uses. This set was created to see if the limitation to 10.000 tokens reduces the amount of noise, increasing the performance. We will refer to this embedding in the figures as the \textbf{10k\_embedding}.
\subsubsubheader{5K TF-IDF embedding}
The 5K TF-IDF embedding is the TF-IDF embedding set, limited to the 5.000 most common words. This set was created to limit the number of tokens more aggressively, and with that, cancel out more noise. We will refer to this embedding in the figures as \textbf{5k\_embedding}.
\subsubsubheader{1K-6K TF-IDF embedding}
The 1K-6K TF-IDF embedding is the TF-IDF embedding limited to the top 6.000 most common words, without the top 1.000 most common words. The rationale for this is that common words will occur in many articles, creating noise, by cutting off the top 1.000 and cutting off everything below 6.000 we tried to reduce the noise by filtering common words. This cut results in a set of 5.000 tokens, which allows us to compare it to the 5K TF-IDF set. We will refer to this embedding in the figures as \textbf{1k\_6k\_embedding}. We refer to the \textbf{10k\_embedding}, \textbf{5k\_embedding} and the \textbf{1k\_6k\_embedding} as the \textbf{limited TF-IDF embeddings}, because these embedding use TF-IDF weighing, and have been limited in their vocabulary due to the cut-off's.
\clearpage
\subsubheader{TF-IDF}
To create the TF-IDF feature vectors, we used the TF-IDF model and a hasher from PySparks MlLib library. The TF-IDF feature vectors are created by hashing the tokens with the hasher, which has a set hash bucket size. These hashed values are passed on to the TF-IDF model, resulting in a feature vector which vector dimensions equal the number of hash buckets. To limit the computational and storage expenses and to reduce noise by rare words, we limit our vocabulary size.
We label the TF-IDF configurations as follows: $vocabulary size/hash bucket size$. Furthermore, we denote 1.000 as 1K, since we deal with chosen values which can be exactly noted given this notation. This means that the set with a 10.000 vocabulary size and a 10.000 hash bucket size will be denoted as "10K/10K". We will refer to the TF-IDF configurations in our figures as "tfidf\_vocabulary size\_hash bucket size".
\begin{figure}[hbt]
\includegraphics[width=5in]{Plots/tfidf_selection_plot_performance}
\caption{TF-IDF performance on title and abstract}\label{figure:tfidfPerformance}
\end{figure}
\begin{figure}[hbt]
\includegraphics[width=5in]{Plots/tfidf_selection_plot_memory}
\caption{TF-IDF memory usage for title and abstract combined}\label{figure:tfidfMemory}
\end{figure}
\FloatBarrier
Figure~\ref{figure:tfidfPerformance} shows the performance of title and abstract as median rank, and Figure~\ref{figure:tfidfMemory} shows the storage size in gigabyte, of the 1k/1K, 4K/4K, 7K/7K and 10K/10K TF-IDF configurations. This plot shows that, while the required storage size stagnate, the performance on title also quickly stagnates, together with the performance on the abstracts. The stagnation of the storage size is likely due to the fact that the terms which are added to the higher vocabulary sets occur less often than the other words (the cut-off is based on word occurrence, descending). Given this information, we have chosen to use the 10K/10K, 10K/5K and 5K/5K configurations to compare our embedding results to. Since these configurations can be kept in memory (storage size) while it is not likely that larger sets will give much improvement. The TF-IDF features are created on article level. We average the set of article feature vectors to create a journal feature vector, just as we did with the embeddings. 
\header{Pipeline}
\subheader{Research environment}
The research has been done in a python-databricks environment, which used Spark, a library that offers tools to work with big-data. \citet{armbrust2015spark} state that Spark SQL lets programmers leverage the benefits of relational processing and lets SQL users call complex analytic libraries in Spark. This allows for much tighter integration between relational and procedural processing. The paper further states that Spark SQL makes it significantly simpler and more efficient to write data pipelines that mix relational and procedural processing while offering substantial speedups over previous SQL-on-Spark engines.\\
We procesessed the TF-IDF sets and the embedding sets via the same pipeline, using their common vector properties. This ensures comparable results, the pipeline is set-up as follows:
\begin{enumerate}
\item{Create training and validation set}
\item{Create journal embeddings}
\item{Categorize validation articles}
\item{Calculate performance metrics}
\end{enumerate}
  
\subheader{Create training and validation set}
We split our initial set  $80\%$ - $20\%$. We use the $80\%$ set as the training set for the journal representations, and the $20\%$ set as the validation set for the journal representations. This split is based on a random number given to each article, ensuring that all set have the same (random) training and validation set.
\subheader{Create journal embeddings}
From our training set we create the journal embeddings, which are created for most sets\footnote{see paragraph datasets} by averaging the article embeddings or feature vectors.
\subheader{Categorize validation articles}
To categorize the articles, we calculate the distance between the title of the article, and the title of each journal from the validation set. We also do this for the abstract of the article and the abstract of each journal. During this process we keep track of:
\begin{itemize}
\item{Title-based-rank of the actual journal}
\item{Abstract-based-rank of the actual journal}
\item{Best scored journal on the abstract similarity}
\item{Best scored journal on the title similarity}
\item{Abstract similarity between the actual journal and the article}
\item{Title similarity between the actual journal and the article}
\end{itemize}
\subsubheader{Distance metrics}
To calculate the distance between two vectors, cosine similarity is commonly used. We validated the quality of cosine similarity as a distance metric by comparing it to other similarity matrices available in the SciPy library\footnote{\url{https://www.scipy.org/}}, which we used to calculate the distances. We calculate the similarities based on the normalized embeddings, and compared the distance metrics based on the default embedding set. Table~\ref{table:distanceMetrics} shows the results of this validation.\\
\begin{table}[hbt]
\begin{tabular}{|l|l|l|l|l|}
\hline
Metric\footnotemark & Median title rank & Average title rank & Median abstract rank & Average abstract rank  \\
\hline
Braycurtis & 28 & 130 & 23 & 124  \\
\hline
Canberra & 33 & 148 & 26 & 133  \\
\hline
Chebyshev & 57 & 256 & 41 & 191  \\
\hline
\textit{Cityblock} & \textit{28} & \textit{130} & \textit{23} & \textit{124}  \\
\hline
\textit{Correlation} & \textit{27} & \textit{127} & \textit{23} & \textit{122}  \\
\hline
\textbf{Cosine} & \textbf{27} & \textbf{127} & \textbf{23} & \textbf{122}  \\
\hline
Dice & 1995 & 1929 & 1995 & 1929  \\
\hline
\textit{Euclidean} & \textit{27} & \textit{127} & \textit{23} & \textit{122}  \\
\hline
Hamming & 1995 & 1929 & 1995 & 1929  \\
\hline
Jaccard & 1995 & 1929 & 1995 & 1929  \\
\hline
Kulsinski & 1995 & 1929 & 1995 & 1929  \\
\hline
Mahalanobis & 136 & 544 & 75 & 449  \\
\hline
Matching & 1995 & 1929 & 1995 & 1929  \\
\hline
Rogerstanimoto & 1995 & 1929 & 1995 & 1929  \\
\hline
Russellrao & 1995 & 1929 & 1995 & 1929  \\
\hline
\textit{Seuclidean} & \textit{27} & \textit{124} & \textit{22} & \textit{115}  \\
\hline
Sokalmichener & 1995 & 1929 & 1995 & 1929  \\
\hline
Sokalsneath & 1995 & 1929 & 1995 & 1929  \\
\hline
\textit{Sqeuclidean} & \textit{27} & \textit{127} & \textit{23} & \textit{122}  \\
\hline
Yule & 1995 & 1929 & 1995 & 1929  \\
\hline
\end{tabular}
\caption{Distance metric performance for word embeddings on the categorization of academic texts}\label{table:distanceMetrics}
\end{table}
\footnotetext{As defined and provided by the SciPy library} 
These results show high similarity between cosine-based metrics (Cosine \& Correlation) and euclidean based metrics (Euclidean, Seuclidean \& Sqeuclidean). This similarity is expected, since the cosine and euclidean distances should yield the same results on normalized sets. The results show that some enhancement on the euclidean algorithm result in slightly improved results, although not significant. Also the Cityblock metric yields results close to the Cosine metric, it has a slightly worse performance, which is also not significant. Because of this, we will use the cosine-similarity as the distance metric, which will make our results better comparable with other work.
\clearpage
\subheader{Performance measurement}
We use multiple metrics to validate the performance of the embedding sets and TF-IDF sets on the categorization task. These metrics are:
\begin{enumerate}
\item{F1-score}
\item{Median \& average rank}
\item{Rank distribution}
\end{enumerate}
\subsubheader{F1-score}
We define the positive \& negative metrics as follows:\\
\begin{jumpin}
$True Positive = $ Articles that are correctly matched to the current journal\\
$False Positive = $ Articles that are incorrectly matched to other journals\\
$False Negative = $ Articles that are incorrectly matched to the current journal\\
\end{jumpin}
We used these metrics to calculate the Recall, Precision \& F1 as follows:\\
\begin{jumpin}
$Recall = \dfrac{True Positive}{True Positive + False Negative}$\vspace{0.1in}\\
$Precision = \dfrac{True Positive}{True Positive + False Positive}$\vspace{0.1in}\\
$F1 = \dfrac{2 * Precision * Recall}{Precision + Recall}$
\end{jumpin}
\subsubheader{Median \& average rank}
The median rank of a journal indicates the point where 50\% of all articles are to the left of a virtual line and 50\% are to the right of this line. The median rank therefore indicates the position of the "normal/typical"  article. This median rank differs from the average, since the median is not influenced by a single value. If the median is 20, it does not matter if an article rank is 25 of 250, it does matter however if an article rank is 19 or 21. For the average this is not the case, the average is influenced by single articles. Therefore, it does not represents the "normal/typical" article but it represents the exact average of all articles in the validation set, which is changed when an article rank is changed from 25 to 250.\\
We use the median rank to indicate at which rank the typical article would be ranked, based on its title and abstract. We do this by taking the median of the respective rank from each article. This gives us an indication of the behaviour of the articles in our validation set. We also calculate the average score for the title and abstract of all articles. This gives us four scores: \textbf{median title rank, median abstract rank, average title rank \& average abstract rank.}\\
\subsubheader{Rank distribution}
To further analyse the ranking results, we plot the rank distribution to get an indication of the ranking-landscape.
\subsubheader{Two-dimensional plot}
To create a plot, we transformed the 300-dimensional journal vectors into 2-dimensional vectors using TSNE based on pca\footnote{Principal component analysis}. These 2-dimensional vectors, representing the x \& y coordinate, can then be drawn in a plot. To visualize the preservation of journal-relatedness while converting the 300 dimensions to 2 dimensions, we create groupings using k-means. These groupings are created on the 300-dimensional vectors and are visualized in the plot using colors. We use the k-means groupings due to the lack of subject-based grouping values in our dataset, for this research we used 8 groups.
\end{document}