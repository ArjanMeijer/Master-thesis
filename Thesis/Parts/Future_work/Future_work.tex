\documentclass[../../Thesis.tex]{subfiles}
\begin{document}
\header{Future work}
This research focused on the quality of word embeddings on academic texts. To do this, we used both a comparison to TF-IDF and a visualization of the word embeddings. Future work may seek to improve the quality of the embeddings further or determine the limit of the capabilities of the embedding technique. 

\subheader{Method differences}
\citet{levy2015improving} observed that there were no significant differences between the various embedding creation methods. They state that the global/hyperparameters of the various methods mainly cause the difference in performance. In this research, we did not look at the comparison between various models and (hyper) parameters. We instead used a standard configuration, to focus more on the actual performance of the embeddings, instead of the best possible performance. Future work could seek to validate the results of \citet{levy2015improving} by expanding our research with multiple metrics. It should be noted however that Levy et al. base their conclusions on word similarity and word analogy tasks, and not on categorization. This is the reason why we did not take this research into account in our discussion since their experiments are not comparable to ours.

\subheader{Intelligent cutting}
A better way of cutting could improve the quality of the embeddings. This improvement might be achieved by cutting the center of the vector space out before normalization. This must be applied before normalization since normalization causes all embeddings to have a distance of 1 to the center point. All words which are generic are in the center of the spectrum before normalization. Removing these words prevents the larger texts to be pulled towards the middle of the vector space, where they lose the parts of their meaning which set them apart from the other journals. We expect that this way of cutting, instead of word-occurrence cutting, will improve the quality of the word embeddings.

\subheader{Text combination}
To cope with the problem of articles that do have an abstract but no title, or vice versa, it would be interesting to see what the quality of the embeddings would be if both texts were combined into one text. This should be possible since the title and the abstract per article share a common topic. We would expect that the common text would have the quality of the abstract, which is according to our findings the part that is best used for the embeddings and TF-IDF.

\subheader{TF-IDFs performance point}
In our research, TF-IDF performed better on the abstracts than on the titles, which, according to us, is caused by the text size of the two texts. This leads to the question, how do token count and unique token count relate for TF-IDF? Is there a point at which the TF-IDF outperforms the embeddings, and will continue to outperform? If this relation is found,  we could skip the TF-IDF calculations in certain situations, and skip the embedding training in other scenario's, saving time and costs.

\subheader{Reversed word pairs}
At this point, there are no domain-specific word pair sets available. However, as we demonstrated, we can still test the quality of word embeddings. Once we established that we have word vectors of high quality, could we create word pairs from the embeddings? If this is the case, we could create word pair sets using the embeddings, reverse-engineering the domain specific word pair sets for future use. These word pairs should most likely still be validated by humans, but the automatic generation of word pairs should already reduce the effort needed for this process.

\subheader{Historical overview}
We have shown that the word embeddings can be used to create a subject spectrum, in which we plotted all the articles for one year. This gives insight into the currently popular research field, indicated by dense areas on the plot. If we would create this plot over multiple years, and then show these years in chronological order, we should be able to see the evaluation of research fields in time, giving more insight in shifting interests and the development of new research areas.

\subheader{TF-IDF top cut-off}
In our research, we used a dataset from which we removed the top 1k words, together with everything beyond 6k. This dataset did not perform well in our research; future work could look into this by validating if the top-words cut-off decreases the TF-IDF performance. Our findings on this topic are minimal, although we can say that we would not expect that the top-cutoff improves the performance since a set with the top 5k words performed better than the 1k till 6k words set.

\subheader{Collecting a set of terms}
Our results show that, for larger texts, TF-IDF outperforms the embeddings. Given this, it would be logical to use TF-IDF for search tasks on sets with many tokens. However, the search queries consist of a small number of tokens (the average google query size was 4 in 2008\footnote{According to Brian Ussery (01/02/2008): \url{https://www.beussery.com/blog/index.php/2008/02/google-average-number-of-words-per-query-have-increased/}.}). Our research showed that embeddings perform better in a situation with a low amount of tokens. Future work could try to combine these findings, by collecting a large number of words, resembling the meaning (captured by the embeddings), and transforming this into a (large) collection of words (mimicking word occurrence, captured by the TF-IDF). If this could be done effectively, the power of the TF-IDF method could be applied to smaller texts. This process could be seen as a translation from the word-embedding space into the TF-IDF space. Furthermore, it would be interesting to see which words would be selected and if these words represent the given sentence as an "extracted version" which would still be interpretable by humans.
\clearpage
\subheader{Pre-categorization}
Our results show that the embeddings can indicate the domains of the given articles, as visualized in the journal embedding plots (Figures~\ref{figure:titlePlotNormal}-\ref{figure:abstractPlotGrouped}). This knowledge could be used to pre-process given texts, creating a two-step search. This method would first categorize the given article into a field and then search in the limited set of journals for the best fit. This has two advantages: (a) we predict the field of the article, giving additional information to a user and (b) once an article is inside a certain domain, ambiguous words between domains (i.e. cloud for computer science and meteorology) are eliminated. The domain specific embeddings have not been trained with the other embeddings, thus they do not contain (cross-domain) ambiguity. A possible downside of this approach is the need for data within each domain, because the embeddings are created on two levels: cross-domain for domain categorization and in-domain for journal categorization. The lack of articles within one domain could hurt the embeddings since they need training data to make accurate predictions. In a situation where there is no categorization data available, like in our research, a k-means algorithm should suffice for categorization. The goal is to have multiple separate vector spaces, for this, it does not matter if the "domains" are well-interpretable or make sense to humans.
\end{document}