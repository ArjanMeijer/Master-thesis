\documentclass[../../Thesis.tex]{subfiles}
\begin{document}
\header{Introduction and Background}
\subheader{Information retrieval}
Information Retrieval(IR)is the activity of gathering relevant information, given another initial piece of information. The most practical example of this is a search engine. Given one or more search words the search engine will attempt to find relevant information. For example, an online search for "Information Retrieval" (initial piece of information) will give you a list of results (relevant information). To be able to do this, the search engine must know which texts are related. This can be achieved with and without neural networks, a traditional technique which does not use a neural network is TF-IDF, shot for Term Frequency - Inverted Document Frequency. Techniques that use a neural network are newer, these are (among others) Word2Vec, Paragraph Vectors and GloVe.

\subheader{Neural Network}
Neural Networks are computational structures, based on vector and/or matrix calculations. They can be applied to many different tasks, but need to be trained for each task. This training is the process of iteratively adjusting multiplication matrices or vectors to achieve the optimal result. Which is the result that is as close as possible to the given output for different input and output sets.
In some IR techniques, Neural Networks are used to either predict words that may occur around a given word, or predict a word given words that surround it. For example, given the sentence\\
\begin{center}
\textit{"The search engine searches for relevant information"}
\end{center}
the Neural Network can be trained to either predict the words around "searches" (the, search, engine, for, relevant, information) or to predict the word "searches", given the surrounding (the, search, engine, for, relevant, information). This trained matrix, a vector per word which is referred to as a word embedding, now indicates "word relatedness" which, as mentioned earlier enables association (thus retrieval) with related(relevant) texts. Neural networks are also used for other tasks such as video recommendation on Google's YouTube, as described by \citet{covington2016deep}.
%The basic theory of word embedding can be explained by the following quote of J.R. Firth: "You shall know a word by the company it keeps". 
\subheader{Embedding}
An embedding is a distributed, numerical representation of a text in a vector space\footnote{The vector space of separately trained word embeddings are not shared, since the trained vector dimensions are not fixed. This means that two separately trained word embeddings which have the same dimension do not share the same vector space.} which can capture both the semantic and syntactic information\cite{mikolov2013distributed}. In the case of a word embedding, the embedding represents the word. These embeddings are created using by machine learning models. These models create the embeddings without the need for human interaction~\cite{lai2016generate}, they are so called \textit{unsupervised learning algorithms}. Once trained, the embeddings can be used to construct embeddings for collections of texts. For example, a word embedding can be used to create a sentence, paragraph, document or corpus embedding. The usage of word embeddings have improved various Natural Language Processing areas such as named entity recognition, part-of-speech tagging, parsing, and semantic role labelling~\citet{luong2013better}.

\subheader{Text analysis techniques}
To enable a computer to process text, for embedding creation or for tasks, the text has to be processed by an algorithm. In this research we used embeddings created by Word2Vec and TF-IDF feature vectors.
\begin{jumpin}
\subsubheader{Word2Vec}
Word2vec word embeddings are created using neural network, Word2vec learns word embeddings via maximizing the log conditional probability of the word given the context word(s) occurring within a fixed-sized window. Therefore the learnt embeddings contain useful knowledge about word co-occurrence\cite{nalisnick2016improving}. There are multiple input/output possibilities for the neural network, best known are Skip-gram and the Continuous Bag-of-Words model (CBOW). The Skip-gram model takes a target word as input and outputs the predicted output words, while CBOW takes the context words as input and outputs the predicted target word\cite{nalisnick2016improving, pennington2014glove}. \citet{mikolov2013distributed}\cite{mikolov2013efficient} presented several extensions to the word2vec model that improve the quality of vector training and its speed, such as introducing Hierarchical Softmax, Negative Sampling and the subsampling of frequent words to the Skip-gram approach. Variations to the word2vec model have also been proposed, such as the doc2vec model described by \citet{lau2016empirical} which creates document embeddings instead of word embeddings.

\subsubheader{Paragraph vectors}
Variations on the word2vec model have also been proposed,~\citet{le2014distributed} introduced the paragraph vector based in their paper "Distributed representations of sentences and documents". The Paragraph Vector framework is based on the word vectors framework. The difference between the frameworks is the calculation of the probability, the Paragraph Vector framework uses an matrix D, which consists of every paragraph. This matrix is used to replace a concatenation or average of word vectors. An advantage of the paragraph vector model is that it takes the word order into consideration, atleast in a small context \cite{le2014distributed}. \citet{dai2015document} state in their paper "Document embedding with paragraph vectors" that the Paragraph Vectors model performs significantly better than other models on grouping, triplet finding and related object/article finding tasks on the Wikipeida and arXiv texts.

\subsubheader{GloVe}
\citet{pennington2014glove} introduced the GloVe (Global Vectors) model. This model captures the global corpus statistics. The model transforms the word co-occurrences of all words in the corpus to chances, it excludes all the zero values and uses that as initial input for the neural network. This model out performs other models on three specific tasks, word analogy, word similarity and entity recognition.\\

\subsubheader{TF-IDF}
TF-IDF is an abbreviation for Term Frequency times Inverted Document Frequency. This method does not rely on a neural network and therefore does not require training. The TF-IDF score is the product of the term frequency in a text and the inverted document frequency of the same term in a corpus of texts. Both of which can be calculated in a variety of ways. The feature vectors produced by TF-IDF do not capture syntactic or semantic information about words, but capture information about word occurrences. These features are created by supplying the algorithm with the text it needs to "know". This text is then analysed on word occurrences on corpus and document and level, this results in a score per-word. This score can be converted to a feature vector, in which the indexes of these vector represent the words and the value is the TF-IDF score. The size of this feature vector can optionally be controlled by hashing the words in the text, which can then be reduced to a given size. If this is not applied, the size of the feature vector is equal to the amount of unique words in the corpus, also known as the (corpus) vocabulary. Some applications of this technique limit the amount of unique words in the text supplied to the TF-IDF algorithm, they do this by taking only a certain amount of top words, ordered on their occurrence. This reduces the amount of storage needed when hashing is not applied. It furthermore reduces the amount of words which occur rarely. Due to the nature of TF-IDF these rare words have a high score, cancelling out other more frequent words, while rarely occurring in the corpus.

\end{jumpin}
As \citet{lai2016generate} state in their paper "How to generate a good word embedding?", all embedding methods rely on the same hypothesis, words that occur in similar contexts have similar meanings. They furthermore found that larger corpus' lead to better quality embeddings, but that the domain in which the embeddings are trained is dominant over the corpus size. 

\subheader{Validation methods}
The results produced by the techniques have to be validated to determine their quality (in usage). The quality of the results can be validated through, among others the F1 score, which combines the precision score and the recall score in a single metric, and, for classification tasks, the rank of the correct category. The embeddings can furthermore be validated through their performance on tasks such as word analogies, word similarities, categorization and position visualization. These tasks can be designed to produce a score that gives an indication of the performance on a specific task. \citet{schnabel2015evaluation} found that a single validation metric cannot produce a representative result for other tasks. Embeddings that perform well on one task do not have to perform well on another task. As a result, the findings about performance of an embedding method are limited to the task on which they are tested, their results cannot be generalized to state that the embedding are overall "performing well". Validation tasks use either labelled or unlabelled data. Labelled data is data that is in some way marked, so that the correct answer can be derived from it, while this is not possible with unlabelled data. Labelled data enables the usage of unsupervised learning models, since  \textit{a system or model which is not involved in the actual learning} can automatically validate the given results and use it as feedback for the next learning iteration. Unlabelled data on the other hand requires human interaction to give a score for the produced result. The validation tasks can also be divided into two groups, extrinsic evaluation, ones that use word embeddings as input for a downstream task. And Intrinsic evaluation, which directly test the relationships of the word embedding themselves. \citet{schnabel2015evaluation} note that the extrinsic evaluation may not be consistent with intrinsic evaluations, since the performance on downstream tasks is not consistent across tasks.

\begin{jumpin}
\subsubheader{Word Analogy}
Word analogy validation is based on a labelled validation set, containing, commonly, word pairs of four, that can be logically divided into two parts. As Table~\ref{table:wordAnalogies} shows, each last word can be derived from the three words before. The score is the fraction of correctly given fourth words, given the first three words. This validation metric is used in multiple studies\cite{mikolov2013distributed, mikolov2013efficient, dai2015document, pennington2014glove}.\\
\begin{table}[hbt]
\begin{center}
\begin{tabular}{l l l l}
Man & Women & King & Queen \\
Athens & Greece & Oslo & Norway\\
great & greater & tough & tougher
\end{tabular}
\end{center}
\caption{Word analogies examples}\label{table:wordAnalogies}
\end{table}\\
Both this validation technique and the Word Similarity technique use vector distance calculations to validate the embeddings, this can therefore also be written as:
\begin{displayquote}
	X\textsubscript{Man} - X\textsubscript{King} $\approx$  X\textsubscript{Women} - X\textsubscript{Queen}
\end{displayquote}
This means that the resulting vector of embedding of "Man" minus the embedding of "King" is approximately the embedding of "Woman" minus the embedding of "Queen". This resulting vector may be close to a vector "monarch" for example.
\subsubheader{Word Similarity}
A method to test the quality of word embeddings is the word similarity test. For these test, the distance between the word embeddings (vectors) is measured and compared to similarity scores defined by humans. Multiple non domain specific validation sets are publicly available including: the Rare-word dataset introduced in the paper "Better Word Representations with Recursive Neural Networks for Morphology" by \citet{luong2013better}, 
the MEN test collection by \citet{EBruniMENCollection} and the WordSimilarity-353 test collection by \citet{EGabrilovichWScollection}.
These sets, among others, have been used in multiple studies of word embeddings\cite{pennington2014glove, mikolov2013efficient}. This validation metric also relies on labelled data.
\subsubheader{Classification}
A classification validation method is a simple task which assigns a label to a text. \citet{lau2016empirical} used data from StackExchange and tried to determine if a pair was a duplicate. In their setup, the text was a pair of texts, and their categories were duplicate and non-duplicate. \citet{le2014distributed} used for their research a dataset of IMDB with 100,000 movie review. They validated their proposed paragraph vector model by determining whether a review was positive or negative.
\subsubheader{Position Visualization}
\citet{dai2015document} and \citet{hinton2003stochastic} mapped their word embeddings from a high dimensional vector to a two dimensional vector to be able to display them in a scatter plot and applied colors to various categories. These categories can be created with labelled or unlabelled data. The advantage of this is that a human can directly see the word distributions, and see if it is distributed in a way that seems logical. It gives furthermore insight in the overall spectrum of the words. However, this representation does not give a score, since it is not a evaluation of the data, but an alternative representation. 
\end{jumpin}
\subheader{General and Domain specific}
Since the word embeddings are created form a given text, these embeddings are bound to the text. All meaning embedded in the word embedding is derived from the original text. Because of this, embeddings can be "Domain specific" meaning that it only knows words (or a specific word) in a certain context. This becomes most clear when faced with words that can have different meanings in different contexts. For this research we categorize the embedding in two categories, general embeddings and domain specific embeddings. The general embeddings are trained on a collection of texts that use common English and contains a wide variety of topics. The domain specific embeddings are trained on a collection of texts that uses uncommon English (i.e. domain specific terms) and/or is limited to a small amount of topics. Given these terms, we regard the embeddings trained on the Wikipedia corpus\cite{lai2016generate, pennington2014glove, dai2015document, lau2016empirical, schnabel2015evaluation} as general, since Wikipedia uses common English and spans a wide range of topics. On the other hand we regard the embeddings created by \citet{Truong2017Thesis} as Domain specific, these embeddings were created on academic articles, which use domain-specific terms and notations and only consists of academic texts, which contains less general/generic words compared to the Wikipedia corpus. The domain specific texts can use the same word for unrelated topics, like clouds in computer science and meteorology.
\subheader{Research environment}
The research will be done in a python-databricks environment, which uses spark, a library that offers tools to work with big-data. \citet{armburst2015} state that Spark SQL lest programmers leverage the benefits of relational processing and lets SQL users call complex analytix libraries in Spark. This allows for much tighter intergration between relational and procedural processing. The paper further states that Spark SQL makes it significanly simpler and more efficient to write data piplines that mix relational and procedural processing, while offering substantial speedups over previous SQL-on-Spark engines.
\subheader{Text properties}
\citet{wiegand2018word} state that the pareto distribution offers a good fit to the word occurrences in natural language. Their models show that, due to the additional parameters in the pareto-III, the tail of the data fits better to the model than the Zipf model. This shows the relation between the word occurrences rank, and the actual occurrences. This kind of word-occurrences distribution holds for many texts, including the writings of William Shakespeare and scientific texts and novels\cite{thurner2015understanding}.
\end{document}