\documentclass[../../Thesis.tex]{subfiles}
\externaldocument{../Research_results/Research_results}
\externaldocument{../Data/Data}
\begin{document}
\header{Discussion}
\subheader{Best performers}
The data, as presented in figures~\ref{figure:titleRanks} \&~\ref{figure:abstractRanks} shows that the 10k/10k set performs better than all other TF-IDF sets, although the difference with the 5k/10k is low, 1  median rank on abstract and 3 median ranks on title. For the embeddings the TF-IDF weighted embedding works better than the others, although it is has is not a significant improvement compared to the default embeddings, which 1 median rank higher on abstract, and equal on title.
\subheader{Sets}
\subsubheader{TF-IDF}
The TF-IDF feature vectors outperform the embeddings on the abstract, while the embeddings outperform the TF-IDF feature vectors on the title. The main difference between the abstract \& title is that the title contains less tokens (see table~\ref{table:corpusSize}), this means that the titles contain less information. Due to this, the TF-IDF method, which is purely based on word-occurrences \& counts has less information on the titles. The TF-IDF method works better on the abstract, which contain more tokens, which improves the differentiating the different abstract. The data furthermore shows that increasing the vocabulary size increases the performance of the TF-IDF, this means that none of the created cut-off's resulted in cutting off noise, the increasing size of the vocabulary only improved the performance in this case. It could be possible that at higher vocabulary sizes the cut-off would result in a sharper signal, we did not looked into this further due to our findings that the TF-IDF performance stagnates (presented in figure~\ref{figure:tfidfPerformance}).
\subsubheader{Embedding}
\subsubsubheader{Limited tfidf embeddings}
The limited TF-IDF embeddings all under perform, compared to the non-limited TF-IDF embedding, on the median and average ranking. Indicating that the noise reduction is too much, and it removes meaningful words. If the noise reduction would be too low, we would only see a slight increase or none at all. However, the rank lowers, indicating the reduction in embedding quality due to missing words. This is in line with what we found with the TF-IDF results, higher vocabulary sizes give better performance. However, figures~\ref{figure:titleDistribution} \&~\ref{figure:abstractDistribution} show that their rank distribution is different from the other embeddings. Their pattern show a decent performance indicates the following pattern: a high/average performance on the top-rankings, an under performance on the middle rankings and  a resulting stack-up of articles with a high-ranking. This is further supported by the TF-IDF score on titles (figure~\ref{figure:titleRanks}), on which the limited TF-IDF embeddings are the top performance. Indicating a better performance on the top-1 articles compared to the other sets.\\This leads us to believe that the cut-off was effective, but that is did not suit our purpose. The cut-off moved the "middle-ranked"  articles to either the high end of the rankings, or the lower end. Resulting in low median and average scores, but in (relatively) high accuracy scores. The reduction in vocabulary size did not reduce the storage size for the embeddings, except for the 1K-6K embedding. This indicates that only the 1K-6K cut actually removes entire titles and/or abstracts, since all vectors are stored as dense-vectors\footnote{Dense vectors are bigger in memory, since they store all their values, including zeros. However they can be processed more efficiently during calculations}. This results in a lower memory requirement.

\subsubheader{TF-IDF \& embeddings}
Our hypothesis on the difference between the TF-IDF and the standard embedding is as follows:\\
The embeddings seem to outperform the TF-IDF in situation when there is little information available, the titles in our case. This indicates that the embeddings store some kind of word meaning that enables them to perform relatively well on the titles. The abstracts on the other hand contain much more information. Our data seems to indicate that the amount of information available in the abstracts enable the TF-IDF to cope with the lack of embedded information. 
If this is the case, we could expect that there would little performance increase on the title when we compare the embeddings to the weighted TF-IDF embeddings, since the TF-IDF lacks the information to perform well. This can be seen in our data, only the average rank increased by 3, indicating that there is a difference between the two embeddings, but not a major one. We would also expect on the abstract an increase in performance, since the TF-IDF has more information in this context. We would expect that the weighting applied by the TF-IDF improves the performance of the embedding by indicating word importance. Our data shows a minor improvement in performance of 1 median rank and 10 average ranks while these improvements cannot be seen as significant, our data at least indicates that weighting the embeddings with TF-IDF values has a positive effect on the embeddings.

\subheader{Memory usage}
Although the TF-IDF outperforms the embeddings on the abstracts, the memory usage of the TF-IDF is higher than the memory usage of the embeddings. The top-performing embedding, TF-IDF weighted embedding, uses 3.13 GB, the top performing TF-IDF, 10K/10K uses 11.61 GB, which is 270.93\% of the storage size needed for the embedding. The closest TF-IDF configuration we used was 1K/1K, which uses 5.13 GB (as displayed in figure~\ref{figure:tfidfPerformance}). This TF-IDF set has a median title rank of 183 and a median abstract rank of 44. Which is significantly worse than the embedding, which also uses less memory. 
\end{document}