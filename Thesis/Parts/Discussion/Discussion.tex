\documentclass[../../Thesis.tex]{subfiles}
\begin{document}
\header{Discussion}
\subheader{Sets}
\subheader{Best performers}
The data shows that the 10k/10k set performs better than all other TF-IDF sets, although the difference with the 5k/10k is low, 1  median rank (7.14\%) on abstract and 3 median ranks (8.57\%) on title. For the embeddings the TF-IDF weighted embedding works better than the others, although it is has is not a significant improvement compared to the default embeddings, which 1 median rank higher on abstract, and equal on title.
\subsubheader{TF-IDF}
The TF-IDF feature vectors outperform the embeddings on the abstract, while the embeddings ourperform the TF-IDF feature vectors on the title. The main difference between the abstract \& title is that the title contains less unique tokens (see XXXXX), this means that the titles contain less unique information, making them more generic. Due to this, the TF-IDF method, which is purely based on word-occurrences \& counts cannot differentiate between the highly similar titles. The TF-IDF method works better on the abstract, which contain more unique words, which help in differentiating the different texts. Furthermore, increasing the vocabulary size increases the performance of the TF-IDF, since the vocabulary size is cut-off at word occurrences, which reduces the amount of unique words. 
\subsubheader{Embedding}
\subsubsubheader{Limited tfidf embeddings}
The limited TF-IDF embeddings all under perform, compared to the non-limited TF-IDF embedding, on the median and average ranking. Indicating that the noise reduction is too much, and it removes meaningful words. If the noise reduction would be too low, we would only see a slight increase or none at all. However, the rank lowers, indicating the reduction in embedding quality due to missing words. However, graphs XXX and XXX show that their rank distribution is different from the other embeddings. Their pattern show a decent performance indicates the following pattern: a high/average performance on the top-rankings, an under performance on the middle rankings and  a resulting stack-up of articles with a high-ranking. This is further supported by the TF-IDF score on titles (Graph XXX), on which the limited TF-IDF embeddings are the top performance. Indicating a better performance on the top-1 articles compared to the other sets.\\This leads us to believe that the cut-off was effective, but that is did not suit our purpose. The cut-off moved the "middle-ranked"  articles to either the high end of the rankings, or the lower end. Resulting in low median and average scores, but in (relatively) high accuracy scores. The reduction in vocabulary size did not reduce the storage size for the embeddings, except for the 1K-6K embedding. This indicates that only the 1K-6K cut actually removes entire titles and/or abstracts, since all vectors are stored as dense-vectors\footnote{Dense vectors are bigger in memory, since they store all their values, including zeros. However they can be processed more efficiently during calculations}. This results in a lower memory requirement.

\subsubheader{TF-IDF \& embeddings}
The difference between the TFIDF weighted embedding and the default embedding can be explained as follows:
The embeddings seem to outperform the TF-IDF in situation when there is little information available, the titles in our case. This indicates that the embeddings store some kind of word meaning that enables them to perform relatively well on the titles. The abstracts on the other hand contain much more information. Our data seems to indicate that the amount of information available in the abstracts enable the TF-IDF to cope with the lack of embedded information. If this is the case, we could expect that there would little performance increase on the title, since the TF-IDF lacks the information to perform well. This can be seen in our data, only the average rank increased by 3, indicating that there is a difference between the two embeddings, but not a major one. We could expect on the abstract an increase in performance, since the TF-IDF has more information in this context. We would expect that the weighting applied by the TF-IDF improves the performance of the embedding by indicating word importance. Our data shows a minor improvement in performance of 1(4.35\%) median rank and 10(6.41\%) average ranks.

\subheader{Memory usage}
Although the TF-IDF outperforms the embeddings on the abstracts, the memory usage of the TF-IDF is higher than the memory usage of the embeddings. The top-performing embedding, TF-IDF weighted embedding, uses 3.13 GB, the top performing TF-IDF, 10K/10K uses 11.61 GB, which is 270.93\% of the storage size needed for the embedding. The closest TF-IDF configuration we used was 1K/1K, which uses 5.13 GB (SEE GRAPH XXX). This TF-IDF set has a median title rank of 183 and a median abstract rank of 44. Which is worse than the embedding, which also uses less memory.
\end{document}