\documentclass[../../Thesis.tex]{subfiles}
\externaldocument{../Research_results/Research_results}
\externaldocument{../Data/Data}
\begin{document}
\header{Discussion}
\subheader{Result analysis}
\begin{jumpin}
\subsubheader{Best performers}
The data, as presented in figures~\ref{figure:titleRanks} \&~\ref{figure:abstractRanks} shows that the 10k/10k set performs better than all other TF-IDF sets, although the difference with the 5k/10k is low, 1  median rank on abstract and 3 median ranks on title. For the embeddings the TF-IDF weighted embedding works better than the others, although it is has is not a significant improvement compared to the default embeddings, which 1 median rank higher on abstract, and equal on title. 

\subsubheader{TF-IDF}
The TF-IDF feature vectors outperform the embeddings on the abstract, while the embeddings outperform the TF-IDF feature vectors on the title. The main difference between the abstract \& title is that the title contains less tokens (see table~\ref{table:corpusSize}), this means that the titles contain less information. Due to this, the TF-IDF method, which is purely based on word-occurrences \& counts has less information on the titles. The TF-IDF method works better on the abstract, which contain more tokens, which improves the differentiating the different abstract. The data furthermore shows that increasing the vocabulary size increases the performance of the TF-IDF, this means that none of the created cut-off's resulted in cutting off noise, the increasing size of the vocabulary only improved the performance in this case. It could be possible that at higher vocabulary sizes the cut-off would result in a sharper signal, we did not looked into this further due to our findings that the TF-IDF performance stagnates (presented in figure~\ref{figure:tfidfPerformance}).

\subsubheader{Limited tfidf embeddings}
The limited TF-IDF embeddings all under perform, compared to the non-limited TF-IDF embedding, on the median and average ranking. Indicating that the noise reduction is too much, and it removes meaningful words. If the noise reduction would be too low, we would only see a slight increase or none at all. However, the rank lowers, indicating the reduction in embedding quality due to missing words. This is in line with what we found with the TF-IDF results, higher vocabulary sizes give better performance. However, figures~\ref{figure:titleDistribution} \&~\ref{figure:abstractDistribution} show that their rank distribution is different from the other embeddings. Their pattern show a decent performance indicates the following pattern: a high/average performance on the top-rankings, an under performance on the middle rankings and  a resulting stack-up of articles with a high-ranking. This is further supported by the TF-IDF score on titles (figure~\ref{figure:titleRanks}), on which the limited TF-IDF embeddings are the top performance. Indicating a better performance on the top-1 articles compared to the other sets.\\This leads us to believe that the cut-off was effective, but that is did not suit our purpose. The cut-off moved the "middle-ranked"  articles to either the high end of the rankings, or the lower end. Resulting in low median and average scores, but in (relatively) high accuracy scores. The reduction in vocabulary size did not reduce the storage size for the embeddings, except for the 1K-6K embedding. This indicates that only the 1K-6K cut actually removes entire titles and/or abstracts, since all vectors are stored as dense-vectors\footnote{Dense vectors are bigger in memory, since they store all their values, including zeros. However they can be processed more efficiently during calculations}. This results in a lower memory requirement.

\subsubheader{TF-IDF \& embeddings}
Our hypothesis on the difference between the TF-IDF and the standard embedding is as follows:\\
The embeddings seem to outperform the TF-IDF in situation when there is little information available, the titles in our case. This indicates that the embeddings store some kind of word meaning that enables them to perform relatively well on the titles. The abstracts on the other hand contain much more information. Our data seems to indicate that the amount of information available in the abstracts enable the TF-IDF to cope with the lack of embedded information. 
If this is the case, we could expect that there would little performance increase on the title when we compare the embeddings to the weighted TF-IDF embeddings, since the TF-IDF lacks the information to perform well. This can be seen in our data, only the average rank increased by 3, indicating that there is a difference between the two embeddings, but not a major one. We would also expect on the abstract an increase in performance, since the TF-IDF has more information in this context. We would expect that the weighting applied by the TF-IDF improves the performance of the embedding by indicating word importance. Our data shows a minor improvement in performance of 1 median rank and 10 average ranks while these improvements cannot be seen as significant, our data at least indicates that weighting the embeddings with TF-IDF values has a positive effect on the embeddings.

\subsubheader{Memory usage}
Although the TF-IDF outperforms the embeddings on the abstracts, the memory usage of the TF-IDF is higher than the memory usage of the embeddings. The top-performing embedding, TF-IDF weighted embedding, uses 3.13 GB, the top performing TF-IDF, 10K/10K uses 11.61 GB, which is 270.93\% of the storage size needed for the embedding. The closest TF-IDF configuration we used was 1K/1K, which uses 5.13 GB (as displayed in figure~\ref{figure:tfidfPerformance}). This TF-IDF set has a median title rank of 183 and a median abstract rank of 44. Which is significantly worse than the embedding, which also uses less memory.

\subsubheader{Journal plots}
Figures~\ref{figure:titlePlotNormal} \&~\ref{figure:abstractPlotNormal} show the journals in the, what we will refer to as "subject spectrum". We do this because the journal embeddings capture journal-relatedness, which leads to the clustering of related articles, which share, in varying degree, the common subject. In this figure, we can see that the publisher Wiley is more active in the right part of the spectrum than the left. We can further see that Nature is far to the center, as we would expect a generic journal to be, but it is on the biology/medicine side of the center (indicated by the position of Cell and Pharmaceutical Research).\\
Figures~\ref{figure:titlePlotGrouped} \&~\ref{figure:abstractPlotGrouped} show the journals, grouped with k-means on the original 300-dimensional embedding vector. In the plot we can see that the original embedding-clustering, as provided by the k-means algorithm, is relatively well preserved, most groups stay clustered together. 
The interactive version shows that the clusterings, as seen on all four figures, are subject based. Journals concerning the same subjects are correctly clustered together, and the journals in between topic clusters are also positioned at logical positions. It furthermore shows that the k-means groupings create an insight in research fields, although it is limited by the amount of groups the k-means algorithm creates. Our findings on the 2-dimensional representation of embeddings are similar to those of \citet{dai2015document}, who plotted 4.4 million Wikipedia articles. The major difference between our plot and their plot is that they focus on just a view subjects in an entire plot, and have access to (human made) partitioning data. They have therefore a more reliable grouping metric.
\end{jumpin}
\subheader{Improvements}
This research shows that, even tough the embeddings are able to capture and preserve relatedness, TF-IDF is able outperform the embeddings. Earlier research already proposed improvements to the word embeddings. \citet{dai2015document} show that the usage of paragraph vectors improve the accuracy of word embeddings with 4.4\% on triplet creation with the wikipedia corpus, and a 3.9\% improvement on the same task based on the arXiv articles. Furthermore, \citet{le2014distributed} show that the usage of paragraph vectors decrease the error rate (positive/negative) with 7.7\% compared to averaging the word embeddings on categorizing text as either positive or negative. While this looks promising, we have to keep in mind that our task differs from earlier tasks, since we do not categorize on 2 categories, but on more than 3k. Still, we would expect an improvement by using paragraph vectors since the classification task if fundamentally the same, only on a much larger scale., which complicates the task due to the "grey areas"  between categories, which increases given more categories. \citet{pennington2014glove} showed that the GloVe model outperforms the CBOW model, which is used in this research, on a word analogy task. \citet{wang2016linked} introduced the Linked Document Embedding method (LDE) method, which makes use of additional information about a document, such as citations. Their research specifically focused on categorizing documents, showed a 5.89\% increase of the micro-F1 score on LDE compared to CBOW, and a 9.11\% increase of the macro-F1 score. We would expect that applying this technique on our dataset would improve our scores, given earlier results on comparable tasks.\\
Even though  much research has been done, we have not been able to find published results which are directly comparable to our results. This is likely due to our high amount of categorization groups, which enabled us to handle our results as a ranking problem, instead of a absolute hit, which has been used in earlier researches\cite{wang2016linked}. Even though we have a F1 score, which indicates performance on the absolute hits, our results are not comparable to other works due to the amount of categories, and their overlapping subjects\footnote{As visualized in the journal embedding plots and discussed earlier}.


\end{document}