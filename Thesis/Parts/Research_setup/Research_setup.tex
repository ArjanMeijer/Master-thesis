\documentclass[../../Thesis.tex]{subfiles}
\begin{document}

\header{Experiment setup}
We used the following pipeline to collect the performance metrics for the categorization task:
\begin{enumerate}
\item{Create embeddings}
\item{Filter articles}
\item{Create training and validation set}
\item{Create journal embeddings}
\item{Categorize validation articles}
\item{Calculate performance metrics}
\end{enumerate}
  
\subheader{Filter articles}
We create our initial set of articles by collecting all articles from the journals that were published in the year 2017, and have at least 200 publications in 2017. This reduces the journal set to $3,759$ thousand journals, resulting in a set of $1,391,543$ million articles ().
\subheader{Create training and validation set}
We split our initial set  $80\%$ - $20\%$,. We use the $80\%$ set as the training set for the journal representations, and the $20\%$ set as the validation set for the journal representations.
\subheader{Create journal embeddings}
From our training set we create the journal embeddings by averaging all title embeddings as the journal title embedding, and by averaging all abstract embeddings as the journal abstract embedding. We also normalized both embeddings. 
\subheader{Categorize validation articles}
To categorize the articles, we calculate the distance between the title- and abstract embedding of each article, from the validation set, to the title- and abstract embedding of each journal. To calculate the distance, we use cosine similarity (as provided by the SciPy library\cite{SciPy}). During this process we keep track of:
\begin{itemize}
\item{Title-based-rank of the actual journal}
\item{Abstract-based-rank of the actual journal}
\item{Best scored journal on the abstract similarity}
\item{Best scored journal on the title similarity}
\item{Abstract similarity between the actual journal and the article}
\item{Title similarity between the actual journal and the article}
\end{itemize}
\subheader{Performance measurement}
We use multiple metrics to indicate the performance of the embeddings on a categorization task. These metrics are:
\begin{enumerate}
\item{F1-score}
\item{Median \& average rank}
\item{Rank distribution}
\end{enumerate}
\subsubheader{F1-score}
We define the positive \& negative metrics as follows:\\
\begin{jumpin}
$True Positive = $ Articles that are correctly matched to the current journal\\
$False Positive = $ Articles that are incorrectly matched to other journals\\
$False Negative = $ Articles that are incorrectly matched to the current journal\\
\end{jumpin}
We used these metrics to calculate the Recall, Precision \& F1 as follows:\\
\begin{jumpin}
$Recall = \dfrac{True Positive}{True Positive + False Negative}$\vspace{0.1in}\\
$Precision = \dfrac{True Positive}{True Positive + False Positive}$\vspace{0.1in}\\
$F1 = \dfrac{2 * Precision * Recall}{Precision + Recall}$
\end{jumpin}
\subsubheader{Median \& average rank}
We use the median rank to indicate around which rank the 'standard' article would be ranked, based on its title or abstract. We do this by taking the median of the respective rank from each article. This gives us an indication of the behaviour of most articles in our validation set. This median rank (mostly) ignores the outliers, we therefore also use the average rank, which gives a more global indication, although this rank may be over-influenced by some outliers.
\subsubheader{Rank distribution}
To further analyse the ranking results, we plot the rank distribution to get an indication of the ranking-landscape. We limit ourselves to the following categories: 1 (absolute hits), top-10, top-20, top-30, top-40, top-50,  top-100 and 100+. 
\end{document}


